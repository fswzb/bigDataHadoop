****** pyspark demo ******

1. Top-N

from pyspark import SparkContextggo
>>> nums = [10, 1, 2, 9, 3, 4, 5, 6, 7]
// parallelize(c, numSlices=None): Distribute a local Python collection to form an RDD.
// takeOrdered(self, num, key=None): Get the N elements from a RDD ordered in ascending order or as specified by the optional key function.
>>> sc.parallelize(nums).takeOrdered(3)
[1, 2, 3]
>>> sc.parallelize(nums).takeOrdered(3, key=lambda x: -x)
[10, 9, 7]
>>> kv = [(10,"z1"), (1,"z2"), (2,"z3"), (9,"z4"), (3,"z5"), (4,"z6"), (5,"z7"), (6,"z8"), (7,"z9")]
>>> sc.parallelize(kv).takeOrdered(3)
[(1, 'z2'), (2, 'z3'), (3, 'z5')]
>>>
>>> sc.parallelize(kv).takeOrdered(3, key=lambda x: -x[0])
[(10, 'z1'), (9, 'z4'), (7, 'z9')]




